{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2016/Week2/file2.txt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py:678: UserWarning: Duplicate names specified. This will raise an error in the future.\n",
      "  return _read(filepath_or_buffer, kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2016/Week3/file3.txt']\n",
      "appended 619537\n",
      "['2016/Week4/file4.txt']\n",
      "appended 968760\n",
      "['2016/Week5/file5.txt']\n",
      "appended 1291069\n",
      "['2016/Week6/file6.txt']\n",
      "appended 1685100\n",
      "['2016/Week7/file7.txt']\n",
      "appended 2088917\n",
      "['2016/Week8/file8.txt']\n",
      "appended 2512923\n",
      "['2016/Week9/file9.txt']\n",
      "appended 2937628\n",
      "['2016/Week10/file10.txt']\n",
      "appended 3394113\n",
      "['2016/Week11/file11.txt']\n",
      "appended 3852849\n",
      "['2016/Week12/file12.txt']\n",
      "appended 4264176\n",
      "['2016/Week13/file13.txt']\n",
      "appended 4660013\n",
      "['2016/Week14/file14.txt']\n",
      "appended 4993958\n",
      "['2016/Week15/file15.txt']\n",
      "appended 5408086\n",
      "['2016/Week16/file16.txt']\n",
      "appended 5852886\n",
      "['2016/Week17/file17.txt']\n",
      "appended 6298087\n",
      "['2016/Week18/file18.txt']\n",
      "appended 6694650\n",
      "['2016/Week19/file19.txt']\n",
      "appended 7132494\n",
      "['2016/Week20/file20.txt']\n",
      "appended 7561155\n",
      "['2016/Week21/file21.txt']\n",
      "appended 7997684\n",
      "['2016/Week22/file22.txt']\n",
      "appended 8416934\n",
      "['2016/Week23/file23.txt']\n",
      "appended 8815724\n",
      "['2016/Week24/file24.txt']\n",
      "appended 9210636\n",
      "['2016/Week25/file25.txt']\n",
      "appended 9568634\n",
      "['2016/Week26/file26.txt']\n",
      "appended 9945009\n",
      "['2018/Week2/file2.txt']\n",
      "['2018/Week3/file3.txt']\n",
      "appended 868736\n",
      "['2018/Week4/file4.txt']\n",
      "appended 1300460\n",
      "['2018/Week5/file5.txt']\n",
      "appended 1796354\n",
      "['2018/Week6/file6.txt']\n",
      "appended 2324951\n",
      "['2018/Week7/file7.txt']\n",
      "appended 2880297\n",
      "['2018/Week8/file8.txt']\n",
      "appended 3442039\n",
      "['2018/Week9/file9.txt']\n",
      "appended 4046025\n",
      "['2018/Week10/file10.txt']\n",
      "appended 4653506\n",
      "['2018/Week11/file11.txt']\n",
      "appended 5206898\n",
      "['2018/Week12/file12.txt']\n",
      "appended 5809370\n",
      "['2018/Week13/file13.txt']\n",
      "appended 6312865\n",
      "['2018/Week14/file14.txt']\n",
      "appended 6752988\n",
      "['2018/Week15/file15.txt']\n",
      "appended 7266594\n",
      "['2018/Week16/file16.txt']\n",
      "appended 7831703\n",
      "['2018/Week17/file17.txt']\n",
      "appended 8348889\n",
      "['2018/Week18/file18.txt']\n",
      "appended 8915349\n",
      "['2018/Week19/file19.txt']\n",
      "appended 9464196\n",
      "['2018/Week20/file20.txt']\n",
      "appended 10012954\n",
      "['2018/Week21/file21.txt']\n",
      "appended 10559773\n",
      "['2018/Week22/file22.txt']\n",
      "appended 11075334\n",
      "['2018/Week23/file23.txt']\n",
      "appended 11579025\n",
      "['2018/Week24/file24.txt']\n",
      "appended 12022994\n",
      "['2018/Week25/file25.txt']\n",
      "appended 12510753\n",
      "['2018/Week26/file26.txt']\n",
      "appended 12990427\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib \n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "def Read_Data(WeekNumber, Year):\n",
    "#\tpass\n",
    "      columns=[\"Mode\", \"Date\", \"DateTime\", \"CardID\", \"CardType\", \"VehicleID\",\n",
    "                   \"ParentRoute\", \"RouteID\", \"StopID\"] \n",
    "\n",
    "     # fname = \"{0}/{1}/{2}/{3}/\".format(path, scan_type, year, week)\n",
    "      fname = glob.glob(Year + '/Week%s' %WeekNumber + \"/*.txt\")\n",
    "      print(fname)\n",
    "      dataset = pd.read_csv(fname[0], sep=\"|\", names=columns)\n",
    "   \n",
    "      # Creating separate columns for Year, Month, Day\n",
    "      dataset['Date']  = pd.to_datetime(dataset['Date'])\n",
    "      dataset['Year']  = dataset['Date'].dt.year\n",
    "      dataset['Month'] = dataset['Date'].dt.month\n",
    "      dataset['Day']   = dataset['Date'].dt.day\n",
    "\n",
    "      #Reading in stoplocation data:\n",
    "      columns2 = [\"StopID\", \"StopName\", \"Bla\", \"Bla\", \"Bla\", \"PostCode\", \"City\",\"Bla\", \"MetroName\", \"lat\", \"long\"]\n",
    "      dataset2 = pd.read_csv('stop_locations.txt', sep=\"|\", names = columns2)\n",
    "      # creating a file with all data and their matched latitued and longitude cooridnates.\n",
    "      newdata =  dataset2[['StopID', 'StopName','PostCode', 'lat', 'long']]\n",
    "      \n",
    "      All = pd.merge(dataset, newdata, how='right', on='StopID')\n",
    "      return All\n",
    "\n",
    "\n",
    "def Append_data(Year, FirstWeek, LastWeek):\n",
    "    basic_table = Read_Data(FirstWeek, Year)\n",
    "    for i in range(FirstWeek+1, LastWeek+1):\n",
    "          Actual_Table    = Read_Data(i, Year)\n",
    "          basic_table     = basic_table.append(Actual_Table) \n",
    "          print('appended', len(basic_table))\n",
    "    return basic_table\n",
    "\n",
    "dataset_2016 = Append_data(\"2016\", 2, 26)\n",
    "dataset_2018 = Append_data(\"2018\", 2, 26)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group(mydata):  \n",
    "    #since groupby gets rid of columns, we are gonna keep the following ones:\n",
    "    dataset_loc       = mydata[[\"StopID\", \"StopName\", \"PostCode\", \"lat\", \"long\"]] \n",
    "    # groupig the dataset by StopID, and counting the number of touch-ons!\n",
    "    dataset_grouped   = mydata.groupby(\"StopID\").CardID.count().reset_index()\n",
    "    #dataset_grouped   = mydata.groupby(\"StopID\").CardID.nunique().reset_index()\n",
    "\n",
    "    All               = pd.merge(dataset_grouped, dataset_loc, how ='left', on = 'StopID')\n",
    "    All.drop_duplicates(subset = \"StopID\", keep='first', inplace=True)\n",
    "    return All\n",
    "\n",
    "All_2016_grouped = group(dataset_2016)\n",
    "All_2018_grouped = group(dataset_2018)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium \n",
    "\n",
    "dataset_2016_2018 = pd.merge(All_2016_grouped , All_2018_grouped , on = \"StopID\" ) \n",
    "# the map will not render if there are too many points\n",
    "# sort it and take the highest 300\n",
    "dataset_2016_2018 = dataset_2016_2018.sort_values(by=[\"CardID_x\"]) \n",
    "dataset_2016_2018 = dataset_2016_2018.tail(300)\n",
    "\n",
    "Delta_R = dataset_2016_2018.CardID_y - dataset_2016_2018.CardID_x #)np.abs(\n",
    "\n",
    "# creating growth and decline datasets\n",
    "dataset_growth  = dataset_2016_2018[Delta_R > 0]\n",
    "dataset_decline = dataset_2016_2018[Delta_R < 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number growth 191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "## growth \n",
    "\n",
    "Delta_R_growth     = dataset_growth.CardID_y - dataset_growth.CardID_x\n",
    "total_number       = Delta_R_growth.count()\n",
    "print('total number growth', total_number)\n",
    "Delta_R_growth_ratio = Delta_R_growth / np.sqrt(total_number) \n",
    "\n",
    "# adding the measured ratio to the growth dataset\n",
    "dataset_growth[\"Delta_R_growth\"] = Delta_R_growth_ratio\n",
    "\n",
    "\n",
    "#creating the folium map\n",
    "m2 = folium.Map(location=[-37.9,145], tiles=\"openstreetmap\",  zoom_start=9.5) #,\n",
    " \n",
    "# I can add marker one by one on the map\n",
    "for i in range(0,len(dataset_growth)):\n",
    "   folium.Circle(\n",
    "      location=[dataset_growth.iloc[i]['lat_x'], dataset_growth.iloc[i]['long_x']],\n",
    "      popup=dataset_growth.iloc[i]['StopName_x'], \n",
    "      radius=dataset_growth.iloc[i]['Delta_R_growth'],\n",
    "      color='blue',\n",
    "      fill=True,\n",
    "      fill_color='blue'\n",
    "   ).add_to(m2)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number decline 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "## decline  \n",
    "\n",
    "Delta_R_decline     = dataset_decline.CardID_x - dataset_decline.CardID_y  \n",
    "total_number        =  Delta_R_decline.count()\n",
    "print('total number decline', total_number)\n",
    "Delta_R_decline_ratio = Delta_R_decline / np.sqrt(total_number) \n",
    "\n",
    "# adding the ratio to the decline dataset\n",
    "dataset_decline[\"Delta_R_decline\"] = Delta_R_decline_ratio \n",
    " \n",
    "# adding these points to the map\n",
    "for i in range(0,len(dataset_decline)):\n",
    "   folium.Circle(\n",
    "      location=[dataset_decline.iloc[i]['lat_x'], dataset_decline.iloc[i]['long_x']],\n",
    "      popup=dataset_decline.iloc[i]['StopName_x'],\n",
    "      radius=dataset_decline.iloc[i]['Delta_R_decline'],\n",
    "      color='red',\n",
    "      fill=True,\n",
    "      fill_color='red'\n",
    "   ).add_to(m2)\n",
    " \n",
    "\n",
    "# Save it as html\n",
    "m2.save('mymap_2016_2018_touchOn_All.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
